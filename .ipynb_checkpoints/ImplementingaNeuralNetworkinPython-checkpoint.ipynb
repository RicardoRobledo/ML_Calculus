{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1264e0-af11-4b98-8838-769b4cb29848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find a small float to avoid division by zero\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "# Sigmoid function and its differentiation\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z.clip(-500, 500)))\n",
    "\n",
    "def dsigmoid(z):\n",
    "    s = sigmoid(z)\n",
    "    return 2 * s * (1-s)\n",
    "\n",
    "# ReLU function and its differentiation\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def drelu(z):\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a442f3-bcc2-4768-80da-399f7e400aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epsilon = 1e-12  # Pequeño valor para evitar log(0) o división por 0\n",
    "\n",
    "# Loss function L(y, yhat) and its differentiation\n",
    "def cross_entropy(y, yhat):\n",
    "    \"\"\"\n",
    "    Binary cross entropy function\n",
    "    L = -y log(yhat) - (1 - y) log(1 - yhat)\n",
    "\n",
    "    Args:\n",
    "        y, yhat (np.array): 1xn matrices where n is the number of data instances\n",
    "\n",
    "    Returns:\n",
    "        average cross entropy value of shape 1x1, averaging over the n instances\n",
    "    \"\"\"\n",
    "    yhat = yhat.clip(epsilon, 1 - epsilon)\n",
    "    return (-(y.T @ np.log(yhat) + (1 - y).T @ np.log(1 - yhat)) / y.shape[1])\n",
    "\n",
    "def d_cross_entropy(y, yhat):\n",
    "    \"\"\"\n",
    "    Derivative of cross entropy with respect to yhat: dL/dyhat\n",
    "    \"\"\"\n",
    "    yhat = yhat.clip(epsilon, 1 - epsilon)\n",
    "    return -np.divide(y, yhat) + np.divide(1 - y, 1 - yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0af0552d-61ac-4f27-8765-310b379d2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class mlp:\n",
    "    \"\"\"Multilayer Perceptron using NumPy\"\"\"\n",
    "    \n",
    "    def __init__(self, layersizes, activations, derivatives, lossderiv):\n",
    "        \"\"\"\n",
    "        Inicializa la configuración de la red sin inicializar pesos ni sesgos.\n",
    "        \n",
    "        Args:\n",
    "            layersizes (list): Lista con el número de neuronas por capa.\n",
    "            activations (list): Lista de funciones de activación por capa.\n",
    "            derivatives (list): Lista de derivadas de activaciones.\n",
    "            lossderiv (function): Derivada de la función de pérdida.\n",
    "        \"\"\"\n",
    "        self.layersizes = layersizes\n",
    "        self.activations = activations\n",
    "        self.derivatives = derivatives\n",
    "        self.lossderiv = lossderiv\n",
    "        \n",
    "        L = len(layersizes)\n",
    "        self.z = [None] * L\n",
    "        self.W = [None] * L\n",
    "        self.b = [None] * L\n",
    "        self.a = [None] * L\n",
    "        self.dz = [None] * L\n",
    "        self.dW = [None] * L\n",
    "        self.db = [None] * L\n",
    "        self.da = [None] * L\n",
    "\n",
    "    def initialize(self, seed=42):\n",
    "        \"\"\"Inicializa los pesos y sesgos con valores aleatorios.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        sigma = 0.1\n",
    "\n",
    "        for l, (n_in, n_out) in enumerate(zip(self.layersizes, self.layersizes[1:]), 1):\n",
    "            self.W[l] = np.random.randn(n_in, n_out) * sigma\n",
    "            self.b[l] = np.random.randn(1, n_out) * sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante (forward pass).\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): Entrada a la red de tamaño (n_instancias, n_features)\n",
    "        \n",
    "        Returns:\n",
    "            Salida de la red neuronal\n",
    "        \"\"\"\n",
    "        self.a[0] = x\n",
    "\n",
    "        for l, func in enumerate(self.activations, 1):\n",
    "            self.z[l] = self.a[l-1] @ self.W[l] + self.b[l]\n",
    "            self.a[l] = func(self.z[l])\n",
    "\n",
    "        return self.a[-1]\n",
    "\n",
    "    def backward(self, y, yhat):\n",
    "        \"\"\"\n",
    "        Propagación hacia atrás (backpropagation).\n",
    "        \n",
    "        Args:\n",
    "            y (np.array): Salida verdadera\n",
    "            yhat (np.array): Salida predicha por la red\n",
    "        \"\"\"\n",
    "\n",
    "        self.da[-1] = self.lossderiv(y, yhat)\n",
    "\n",
    "        for l, func in reversed(list(enumerate(self.derivatives, 1))):\n",
    "            self.dz[l] = self.da[l] * func(self.z[l])\n",
    "            self.dW[l] = self.a[l-1].T @ self.dz[l]\n",
    "            self.db[l] = np.mean(self.dz[l], axis=0, keepdims=True)\n",
    "            self.da[l-1] = self.dz[l] @ self.W[l].T\n",
    "\n",
    "    def update(self, eta):\n",
    "        \"\"\"\n",
    "        Actualiza los pesos y sesgos con descenso del gradiente.\n",
    "        \n",
    "        Args:\n",
    "            eta (float): Tasa de aprendizaje\n",
    "        \"\"\"\n",
    "        for l in range(1, len(self.W)):\n",
    "            self.W[l] -= eta * self.dW[l]\n",
    "            self.b[l] -= eta * self.db[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "53416af0-b755-4a10-90f0-56cd025fd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make data: Two circles on x-y plane as a classification problem\n",
    "X, y = make_circles(n_samples=1000, factor=0.5, noise=0.1)\n",
    "y = y.reshape(-1,1) # our model expects a 2D array of (n_sample, n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a00319d-d228-49bd-a025-325f83f1bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, array([[ 0.00021357,  0.07463643, -0.19581836, -0.0682563 ],\n",
      "       [-0.046233  ,  0.02507806,  0.13704659,  0.09943072],\n",
      "       [-0.02309038,  0.05123046, -0.03799564,  0.00863654],\n",
      "       ...,\n",
      "       [-0.02990578,  0.04890731, -0.02031112,  0.00868278],\n",
      "       [-0.00454518,  0.05837392, -0.09129207,  0.00443129],\n",
      "       [-0.00150395,  0.05357837, -0.06246294,  0.03335386]]), array([[-0.05862982,  0.00349199, -0.11279077],\n",
      "       [-0.07053745, -0.01013143, -0.10839144],\n",
      "       [-0.05751389,  0.0059618 , -0.11471994],\n",
      "       ...,\n",
      "       [-0.05738431,  0.00619741, -0.11479954],\n",
      "       [-0.05782061,  0.00520989, -0.11389632],\n",
      "       [-0.05820397,  0.00589091, -0.11816776]]), array([[-0.0603804 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06052875],\n",
      "       [-0.06045002],\n",
      "       [-0.06017066],\n",
      "       [-0.06035159],\n",
      "       [-0.06040606],\n",
      "       [-0.06034423],\n",
      "       [-0.06046445],\n",
      "       [-0.06017066],\n",
      "       [-0.06040993],\n",
      "       [-0.06047198],\n",
      "       [-0.06044552],\n",
      "       [-0.06033529],\n",
      "       [-0.06017066],\n",
      "       [-0.06055018],\n",
      "       [-0.06017066],\n",
      "       [-0.06051093],\n",
      "       [-0.06052999],\n",
      "       [-0.06056247],\n",
      "       [-0.06017066],\n",
      "       [-0.06019692],\n",
      "       [-0.06017066],\n",
      "       [-0.06037584],\n",
      "       [-0.06056407],\n",
      "       [-0.0605219 ],\n",
      "       [-0.06051205],\n",
      "       [-0.06037678],\n",
      "       [-0.06055802],\n",
      "       [-0.06044378],\n",
      "       [-0.0604132 ],\n",
      "       [-0.06056017],\n",
      "       [-0.06026089],\n",
      "       [-0.0605167 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06047611],\n",
      "       [-0.06046695],\n",
      "       [-0.06034946],\n",
      "       [-0.06048615],\n",
      "       [-0.06034608],\n",
      "       [-0.06053826],\n",
      "       [-0.06034217],\n",
      "       [-0.06047273],\n",
      "       [-0.06048739],\n",
      "       [-0.06052364],\n",
      "       [-0.06040859],\n",
      "       [-0.06050876],\n",
      "       [-0.06017066],\n",
      "       [-0.06051465],\n",
      "       [-0.06053761],\n",
      "       [-0.06045228],\n",
      "       [-0.06039581],\n",
      "       [-0.06039426],\n",
      "       [-0.06042816],\n",
      "       [-0.06041746],\n",
      "       [-0.06048483],\n",
      "       [-0.06017066],\n",
      "       [-0.06035481],\n",
      "       [-0.06051377],\n",
      "       [-0.06017066],\n",
      "       [-0.06044672],\n",
      "       [-0.06050233],\n",
      "       [-0.06053093],\n",
      "       [-0.06039689],\n",
      "       [-0.06033948],\n",
      "       [-0.06045352],\n",
      "       [-0.06041594],\n",
      "       [-0.06017066],\n",
      "       [-0.0603527 ],\n",
      "       [-0.06051266],\n",
      "       [-0.06044224],\n",
      "       [-0.06017066],\n",
      "       [-0.06041343],\n",
      "       [-0.06044768],\n",
      "       [-0.06026949],\n",
      "       [-0.06035603],\n",
      "       [-0.0605191 ],\n",
      "       [-0.06048062],\n",
      "       [-0.06049841],\n",
      "       [-0.06043778],\n",
      "       [-0.06051781],\n",
      "       [-0.06033027],\n",
      "       [-0.06056016],\n",
      "       [-0.06037416],\n",
      "       [-0.06042069],\n",
      "       [-0.06049059],\n",
      "       [-0.06043829],\n",
      "       [-0.06036035],\n",
      "       [-0.06033871],\n",
      "       [-0.06037233],\n",
      "       [-0.06017066],\n",
      "       [-0.06049995],\n",
      "       [-0.06037892],\n",
      "       [-0.06050128],\n",
      "       [-0.06049156],\n",
      "       [-0.06049115],\n",
      "       [-0.06036951],\n",
      "       [-0.06017066],\n",
      "       [-0.06021964],\n",
      "       [-0.06048785],\n",
      "       [-0.06042741],\n",
      "       [-0.06017066],\n",
      "       [-0.06042284],\n",
      "       [-0.06019265],\n",
      "       [-0.06017066],\n",
      "       [-0.06057271],\n",
      "       [-0.06043877],\n",
      "       [-0.06034875],\n",
      "       [-0.06049938],\n",
      "       [-0.06039113],\n",
      "       [-0.06017066],\n",
      "       [-0.06045914],\n",
      "       [-0.06043491],\n",
      "       [-0.06055975],\n",
      "       [-0.06048333],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06044856],\n",
      "       [-0.06047848],\n",
      "       [-0.06034255],\n",
      "       [-0.0604012 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06042617],\n",
      "       [-0.06053176],\n",
      "       [-0.06040588],\n",
      "       [-0.06046219],\n",
      "       [-0.06041523],\n",
      "       [-0.0605197 ],\n",
      "       [-0.06024361],\n",
      "       [-0.0604161 ],\n",
      "       [-0.06045869],\n",
      "       [-0.06041905],\n",
      "       [-0.06052547],\n",
      "       [-0.06035204],\n",
      "       [-0.06044773],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06022432],\n",
      "       [-0.06032479],\n",
      "       [-0.06017066],\n",
      "       [-0.06050423],\n",
      "       [-0.06047863],\n",
      "       [-0.06027193],\n",
      "       [-0.06035305],\n",
      "       [-0.06023704],\n",
      "       [-0.0605203 ],\n",
      "       [-0.06046502],\n",
      "       [-0.06047511],\n",
      "       [-0.06029677],\n",
      "       [-0.06023767],\n",
      "       [-0.06017066],\n",
      "       [-0.0603941 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06047366],\n",
      "       [-0.06025256],\n",
      "       [-0.06036259],\n",
      "       [-0.06017066],\n",
      "       [-0.06031313],\n",
      "       [-0.06031445],\n",
      "       [-0.06017066],\n",
      "       [-0.06053812],\n",
      "       [-0.06036161],\n",
      "       [-0.06044969],\n",
      "       [-0.06052942],\n",
      "       [-0.06046306],\n",
      "       [-0.06017066],\n",
      "       [-0.06052625],\n",
      "       [-0.06048312],\n",
      "       [-0.06045253],\n",
      "       [-0.06039274],\n",
      "       [-0.06044508],\n",
      "       [-0.06047427],\n",
      "       [-0.06050756],\n",
      "       [-0.06039837],\n",
      "       [-0.06044964],\n",
      "       [-0.06052643],\n",
      "       [-0.06036078],\n",
      "       [-0.06041703],\n",
      "       [-0.06047508],\n",
      "       [-0.06038955],\n",
      "       [-0.06032336],\n",
      "       [-0.06047271],\n",
      "       [-0.06034066],\n",
      "       [-0.06049017],\n",
      "       [-0.06051131],\n",
      "       [-0.0602868 ],\n",
      "       [-0.06041448],\n",
      "       [-0.06043933],\n",
      "       [-0.06039048],\n",
      "       [-0.06050748],\n",
      "       [-0.06022281],\n",
      "       [-0.06044664],\n",
      "       [-0.0604475 ],\n",
      "       [-0.06032397],\n",
      "       [-0.0604829 ],\n",
      "       [-0.06044435],\n",
      "       [-0.06017066],\n",
      "       [-0.0604309 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.0604429 ],\n",
      "       [-0.06023253],\n",
      "       [-0.06017066],\n",
      "       [-0.06043778],\n",
      "       [-0.06017066],\n",
      "       [-0.06042777],\n",
      "       [-0.06051509],\n",
      "       [-0.06051073],\n",
      "       [-0.06036583],\n",
      "       [-0.06034289],\n",
      "       [-0.06047693],\n",
      "       [-0.06041253],\n",
      "       [-0.06036041],\n",
      "       [-0.06045452],\n",
      "       [-0.06057451],\n",
      "       [-0.06053307],\n",
      "       [-0.06017066],\n",
      "       [-0.0605155 ],\n",
      "       [-0.06051133],\n",
      "       [-0.0604952 ],\n",
      "       [-0.06046795],\n",
      "       [-0.06022621],\n",
      "       [-0.0603223 ],\n",
      "       [-0.06024968],\n",
      "       [-0.06041349],\n",
      "       [-0.06017066],\n",
      "       [-0.0605    ],\n",
      "       [-0.06050701],\n",
      "       [-0.06017066],\n",
      "       [-0.06041463],\n",
      "       [-0.06045694],\n",
      "       [-0.06043987],\n",
      "       [-0.06050775],\n",
      "       [-0.06050967],\n",
      "       [-0.06052131],\n",
      "       [-0.06043321],\n",
      "       [-0.06043941],\n",
      "       [-0.06038673],\n",
      "       [-0.06034587],\n",
      "       [-0.06041189],\n",
      "       [-0.0603768 ],\n",
      "       [-0.0603341 ],\n",
      "       [-0.06034264],\n",
      "       [-0.06058727],\n",
      "       [-0.06030447],\n",
      "       [-0.06048105],\n",
      "       [-0.06017066],\n",
      "       [-0.06040281],\n",
      "       [-0.06055281],\n",
      "       [-0.06055858],\n",
      "       [-0.06041947],\n",
      "       [-0.06044724],\n",
      "       [-0.06038479],\n",
      "       [-0.06037686],\n",
      "       [-0.06039407],\n",
      "       [-0.06034787],\n",
      "       [-0.06044971],\n",
      "       [-0.06054747],\n",
      "       [-0.06043776],\n",
      "       [-0.06017066],\n",
      "       [-0.06036575],\n",
      "       [-0.06043311],\n",
      "       [-0.06037138],\n",
      "       [-0.06023492],\n",
      "       [-0.06039254],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06043265],\n",
      "       [-0.0603847 ],\n",
      "       [-0.06051706],\n",
      "       [-0.06042996],\n",
      "       [-0.06034963],\n",
      "       [-0.06043816],\n",
      "       [-0.06050169],\n",
      "       [-0.06017066],\n",
      "       [-0.06037849],\n",
      "       [-0.06047485],\n",
      "       [-0.06041602],\n",
      "       [-0.06051901],\n",
      "       [-0.06033964],\n",
      "       [-0.06048068],\n",
      "       [-0.06046034],\n",
      "       [-0.060358  ],\n",
      "       [-0.0605255 ],\n",
      "       [-0.06035983],\n",
      "       [-0.06041739],\n",
      "       [-0.06049863],\n",
      "       [-0.06043678],\n",
      "       [-0.06031525],\n",
      "       [-0.06057908],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06042041],\n",
      "       [-0.06050496],\n",
      "       [-0.06038654],\n",
      "       [-0.06038619],\n",
      "       [-0.06033388],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06037526],\n",
      "       [-0.06017066],\n",
      "       [-0.06047466],\n",
      "       [-0.06044823],\n",
      "       [-0.0605476 ],\n",
      "       [-0.06040507],\n",
      "       [-0.06043756],\n",
      "       [-0.06043772],\n",
      "       [-0.06050695],\n",
      "       [-0.06024767],\n",
      "       [-0.06034615],\n",
      "       [-0.06050619],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06033961],\n",
      "       [-0.0605394 ],\n",
      "       [-0.06034922],\n",
      "       [-0.0603327 ],\n",
      "       [-0.06047717],\n",
      "       [-0.06037606],\n",
      "       [-0.06040761],\n",
      "       [-0.06056798],\n",
      "       [-0.06051524],\n",
      "       [-0.06036748],\n",
      "       [-0.06045542],\n",
      "       [-0.0604821 ],\n",
      "       [-0.06037873],\n",
      "       [-0.06017066],\n",
      "       [-0.06046462],\n",
      "       [-0.06043103],\n",
      "       [-0.0604055 ],\n",
      "       [-0.0603523 ],\n",
      "       [-0.06041823],\n",
      "       [-0.06035232],\n",
      "       [-0.06037161],\n",
      "       [-0.06050938],\n",
      "       [-0.06045734],\n",
      "       [-0.06017066],\n",
      "       [-0.06037901],\n",
      "       [-0.0603874 ],\n",
      "       [-0.06045117],\n",
      "       [-0.06043783],\n",
      "       [-0.06033237],\n",
      "       [-0.0604816 ],\n",
      "       [-0.06053171],\n",
      "       [-0.0603418 ],\n",
      "       [-0.06056588],\n",
      "       [-0.06050104],\n",
      "       [-0.06048094],\n",
      "       [-0.06042356],\n",
      "       [-0.06043781],\n",
      "       [-0.06028186],\n",
      "       [-0.06030808],\n",
      "       [-0.06041317],\n",
      "       [-0.06045001],\n",
      "       [-0.06035594],\n",
      "       [-0.06037819],\n",
      "       [-0.06055024],\n",
      "       [-0.06042851],\n",
      "       [-0.06039977],\n",
      "       [-0.06017066],\n",
      "       [-0.06035045],\n",
      "       [-0.06045754],\n",
      "       [-0.06049032],\n",
      "       [-0.06034522],\n",
      "       [-0.0602335 ],\n",
      "       [-0.06047075],\n",
      "       [-0.06039839],\n",
      "       [-0.06036037],\n",
      "       [-0.06043883],\n",
      "       [-0.0604922 ],\n",
      "       [-0.06026013],\n",
      "       [-0.06047939],\n",
      "       [-0.06056462],\n",
      "       [-0.06017066],\n",
      "       [-0.06028756],\n",
      "       [-0.06046763],\n",
      "       [-0.06048136],\n",
      "       [-0.06017066],\n",
      "       [-0.06046852],\n",
      "       [-0.06044518],\n",
      "       [-0.0604603 ],\n",
      "       [-0.06044873],\n",
      "       [-0.06041272],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06035164],\n",
      "       [-0.06040883],\n",
      "       [-0.06043094],\n",
      "       [-0.06030919],\n",
      "       [-0.06042557],\n",
      "       [-0.06017066],\n",
      "       [-0.06040864],\n",
      "       [-0.06017066],\n",
      "       [-0.06032028],\n",
      "       [-0.06034231],\n",
      "       [-0.06046049],\n",
      "       [-0.06053   ],\n",
      "       [-0.06058233],\n",
      "       [-0.06045161],\n",
      "       [-0.06045324],\n",
      "       [-0.06041543],\n",
      "       [-0.06037223],\n",
      "       [-0.06038594],\n",
      "       [-0.0603452 ],\n",
      "       [-0.06047827],\n",
      "       [-0.06030266],\n",
      "       [-0.06036488],\n",
      "       [-0.06018781],\n",
      "       [-0.0604189 ],\n",
      "       [-0.06045187],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06040847],\n",
      "       [-0.0604418 ],\n",
      "       [-0.06018851],\n",
      "       [-0.06055092],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06020017],\n",
      "       [-0.06035721],\n",
      "       [-0.06036228],\n",
      "       [-0.06047401],\n",
      "       [-0.06050318],\n",
      "       [-0.06017066],\n",
      "       [-0.06038258],\n",
      "       [-0.06017066],\n",
      "       [-0.06035391],\n",
      "       [-0.06054482],\n",
      "       [-0.06035761],\n",
      "       [-0.06017066],\n",
      "       [-0.06054559],\n",
      "       [-0.0604074 ],\n",
      "       [-0.0604993 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06037887],\n",
      "       [-0.06056201],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06053366],\n",
      "       [-0.06052365],\n",
      "       [-0.06047065],\n",
      "       [-0.06042501],\n",
      "       [-0.06034681],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06049904],\n",
      "       [-0.06017066],\n",
      "       [-0.06026471],\n",
      "       [-0.06032425],\n",
      "       [-0.060493  ],\n",
      "       [-0.06032693],\n",
      "       [-0.06049921],\n",
      "       [-0.06054878],\n",
      "       [-0.06017066],\n",
      "       [-0.06048848],\n",
      "       [-0.06025747],\n",
      "       [-0.06049132],\n",
      "       [-0.06017066],\n",
      "       [-0.06044347],\n",
      "       [-0.06043658],\n",
      "       [-0.06047862],\n",
      "       [-0.06049956],\n",
      "       [-0.06051048],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06058936],\n",
      "       [-0.06045517],\n",
      "       [-0.06036066],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06053914],\n",
      "       [-0.06017066],\n",
      "       [-0.06043782],\n",
      "       [-0.06017066],\n",
      "       [-0.0604451 ],\n",
      "       [-0.06045252],\n",
      "       [-0.06045691],\n",
      "       [-0.06043404],\n",
      "       [-0.06045422],\n",
      "       [-0.06041213],\n",
      "       [-0.06023804],\n",
      "       [-0.06044261],\n",
      "       [-0.06050525],\n",
      "       [-0.06025295],\n",
      "       [-0.06050561],\n",
      "       [-0.0604546 ],\n",
      "       [-0.06037193],\n",
      "       [-0.06035632],\n",
      "       [-0.06017066],\n",
      "       [-0.06033275],\n",
      "       [-0.06022065],\n",
      "       [-0.06055005],\n",
      "       [-0.06051305],\n",
      "       [-0.06056587],\n",
      "       [-0.06039273],\n",
      "       [-0.0602793 ],\n",
      "       [-0.06042734],\n",
      "       [-0.06045794],\n",
      "       [-0.06046037],\n",
      "       [-0.06038499],\n",
      "       [-0.06028238],\n",
      "       [-0.06017066],\n",
      "       [-0.0603376 ],\n",
      "       [-0.06043168],\n",
      "       [-0.06041727],\n",
      "       [-0.06051774],\n",
      "       [-0.06054891],\n",
      "       [-0.06037428],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06037345],\n",
      "       [-0.06049792],\n",
      "       [-0.06039706],\n",
      "       [-0.06035129],\n",
      "       [-0.06046071],\n",
      "       [-0.06031316],\n",
      "       [-0.06017066],\n",
      "       [-0.06043674],\n",
      "       [-0.06050603],\n",
      "       [-0.06051592],\n",
      "       [-0.06040796],\n",
      "       [-0.06053168],\n",
      "       [-0.06047205],\n",
      "       [-0.06037848],\n",
      "       [-0.06040096],\n",
      "       [-0.06017066],\n",
      "       [-0.06048304],\n",
      "       [-0.0604217 ],\n",
      "       [-0.06050416],\n",
      "       [-0.06017066],\n",
      "       [-0.06037998],\n",
      "       [-0.06017066],\n",
      "       [-0.06054555],\n",
      "       [-0.06052388],\n",
      "       [-0.06039613],\n",
      "       [-0.06044513],\n",
      "       [-0.06031298],\n",
      "       [-0.06038802],\n",
      "       [-0.06045968],\n",
      "       [-0.06019737],\n",
      "       [-0.06017066],\n",
      "       [-0.06040843],\n",
      "       [-0.06043343],\n",
      "       [-0.06038905],\n",
      "       [-0.06046365],\n",
      "       [-0.06051633],\n",
      "       [-0.06055052],\n",
      "       [-0.06050106],\n",
      "       [-0.06042633],\n",
      "       [-0.06039747],\n",
      "       [-0.06032744],\n",
      "       [-0.06022136],\n",
      "       [-0.06050795],\n",
      "       [-0.06034856],\n",
      "       [-0.06043374],\n",
      "       [-0.06046745],\n",
      "       [-0.0602896 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06048607],\n",
      "       [-0.06017066],\n",
      "       [-0.06036372],\n",
      "       [-0.06049946],\n",
      "       [-0.06057605],\n",
      "       [-0.06046459],\n",
      "       [-0.06031019],\n",
      "       [-0.06035531],\n",
      "       [-0.06022916],\n",
      "       [-0.06046539],\n",
      "       [-0.06051994],\n",
      "       [-0.06052485],\n",
      "       [-0.06017066],\n",
      "       [-0.06028426],\n",
      "       [-0.06017066],\n",
      "       [-0.06048019],\n",
      "       [-0.06036805],\n",
      "       [-0.06044105],\n",
      "       [-0.06042999],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06043526],\n",
      "       [-0.06034193],\n",
      "       [-0.06045186],\n",
      "       [-0.06034382],\n",
      "       [-0.0603584 ],\n",
      "       [-0.06044534],\n",
      "       [-0.06058583],\n",
      "       [-0.06029911],\n",
      "       [-0.06017066],\n",
      "       [-0.06047462],\n",
      "       [-0.06055525],\n",
      "       [-0.06036236],\n",
      "       [-0.06048252],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06037985],\n",
      "       [-0.06045674],\n",
      "       [-0.06017066],\n",
      "       [-0.06047739],\n",
      "       [-0.06050797],\n",
      "       [-0.06033634],\n",
      "       [-0.06040049],\n",
      "       [-0.06017066],\n",
      "       [-0.06034316],\n",
      "       [-0.06051739],\n",
      "       [-0.06017066],\n",
      "       [-0.06046316],\n",
      "       [-0.06045663],\n",
      "       [-0.06044134],\n",
      "       [-0.06042794],\n",
      "       [-0.06038754],\n",
      "       [-0.06044441],\n",
      "       [-0.06034244],\n",
      "       [-0.06032411],\n",
      "       [-0.06041367],\n",
      "       [-0.06037059],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06041684],\n",
      "       [-0.06043324],\n",
      "       [-0.06017066],\n",
      "       [-0.06034379],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06049997],\n",
      "       [-0.06041136],\n",
      "       [-0.06041784],\n",
      "       [-0.06033023],\n",
      "       [-0.06021269],\n",
      "       [-0.06043152],\n",
      "       [-0.0604032 ],\n",
      "       [-0.06043765],\n",
      "       [-0.06041807],\n",
      "       [-0.0603612 ],\n",
      "       [-0.06042853],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06044603],\n",
      "       [-0.06045107],\n",
      "       [-0.06049116],\n",
      "       [-0.06044449],\n",
      "       [-0.0604807 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06042535],\n",
      "       [-0.06034859],\n",
      "       [-0.06034824],\n",
      "       [-0.06022507],\n",
      "       [-0.06031521],\n",
      "       [-0.06040403],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06033752],\n",
      "       [-0.06040478],\n",
      "       [-0.06050008],\n",
      "       [-0.06042684],\n",
      "       [-0.06047156],\n",
      "       [-0.06017066],\n",
      "       [-0.06054039],\n",
      "       [-0.06017066],\n",
      "       [-0.06020442],\n",
      "       [-0.06036816],\n",
      "       [-0.06042674],\n",
      "       [-0.06042979],\n",
      "       [-0.06041378],\n",
      "       [-0.06049897],\n",
      "       [-0.0605249 ],\n",
      "       [-0.06055368],\n",
      "       [-0.06034555],\n",
      "       [-0.06017066],\n",
      "       [-0.0604987 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06049536],\n",
      "       [-0.06049722],\n",
      "       [-0.06017066],\n",
      "       [-0.06057939],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06045042],\n",
      "       [-0.06042814],\n",
      "       [-0.06048311],\n",
      "       [-0.06044264],\n",
      "       [-0.0605479 ],\n",
      "       [-0.06037473],\n",
      "       [-0.06045995],\n",
      "       [-0.06048069],\n",
      "       [-0.06046857],\n",
      "       [-0.06022047],\n",
      "       [-0.06040628],\n",
      "       [-0.06032734],\n",
      "       [-0.06032522],\n",
      "       [-0.060364  ],\n",
      "       [-0.06047613],\n",
      "       [-0.06017066],\n",
      "       [-0.06054131],\n",
      "       [-0.06017066],\n",
      "       [-0.06034797],\n",
      "       [-0.06045716],\n",
      "       [-0.06050586],\n",
      "       [-0.06052493],\n",
      "       [-0.06045883],\n",
      "       [-0.06043566],\n",
      "       [-0.06045929],\n",
      "       [-0.0603694 ],\n",
      "       [-0.0604682 ],\n",
      "       [-0.06049775],\n",
      "       [-0.06048839],\n",
      "       [-0.06039243],\n",
      "       [-0.06045995],\n",
      "       [-0.06033711],\n",
      "       [-0.06017066],\n",
      "       [-0.06052241],\n",
      "       [-0.06027953],\n",
      "       [-0.0605113 ],\n",
      "       [-0.06046037],\n",
      "       [-0.06017066],\n",
      "       [-0.06045956],\n",
      "       [-0.06046345],\n",
      "       [-0.06042052],\n",
      "       [-0.06017066],\n",
      "       [-0.06045926],\n",
      "       [-0.06044643],\n",
      "       [-0.06017066],\n",
      "       [-0.0604607 ],\n",
      "       [-0.06038887],\n",
      "       [-0.06043479],\n",
      "       [-0.06025543],\n",
      "       [-0.0604442 ],\n",
      "       [-0.06035693],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06039169],\n",
      "       [-0.06017066],\n",
      "       [-0.06050324],\n",
      "       [-0.06033983],\n",
      "       [-0.06044625],\n",
      "       [-0.060403  ],\n",
      "       [-0.06045112],\n",
      "       [-0.06044518],\n",
      "       [-0.06017066],\n",
      "       [-0.06044954],\n",
      "       [-0.06038073],\n",
      "       [-0.06037778],\n",
      "       [-0.06037183],\n",
      "       [-0.06055544],\n",
      "       [-0.06024033],\n",
      "       [-0.06036046],\n",
      "       [-0.06044517],\n",
      "       [-0.06045162],\n",
      "       [-0.06039851],\n",
      "       [-0.06041465],\n",
      "       [-0.0603305 ],\n",
      "       [-0.06040326],\n",
      "       [-0.06017066],\n",
      "       [-0.06044322],\n",
      "       [-0.06026709],\n",
      "       [-0.06051461],\n",
      "       [-0.06030001],\n",
      "       [-0.06039597],\n",
      "       [-0.06047857],\n",
      "       [-0.06048432],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06050806],\n",
      "       [-0.06034077],\n",
      "       [-0.06043059],\n",
      "       [-0.06050527],\n",
      "       [-0.06050282],\n",
      "       [-0.06029967],\n",
      "       [-0.06056408],\n",
      "       [-0.06034849],\n",
      "       [-0.06036992],\n",
      "       [-0.06017066],\n",
      "       [-0.06049161],\n",
      "       [-0.0601884 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06046383],\n",
      "       [-0.0605646 ],\n",
      "       [-0.06052409],\n",
      "       [-0.06031551],\n",
      "       [-0.06038038],\n",
      "       [-0.06036031],\n",
      "       [-0.06044183],\n",
      "       [-0.06038893],\n",
      "       [-0.06017066],\n",
      "       [-0.06023701],\n",
      "       [-0.06050236],\n",
      "       [-0.06042545],\n",
      "       [-0.06044557],\n",
      "       [-0.06037838],\n",
      "       [-0.06048028],\n",
      "       [-0.06045637],\n",
      "       [-0.06050777],\n",
      "       [-0.06041026],\n",
      "       [-0.0605312 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06043602],\n",
      "       [-0.06032845],\n",
      "       [-0.06045587],\n",
      "       [-0.06042148],\n",
      "       [-0.0604733 ],\n",
      "       [-0.06052498],\n",
      "       [-0.06047574],\n",
      "       [-0.06042368],\n",
      "       [-0.06052666],\n",
      "       [-0.06053356],\n",
      "       [-0.0604418 ],\n",
      "       [-0.06054955],\n",
      "       [-0.06037439],\n",
      "       [-0.06048757],\n",
      "       [-0.06054928],\n",
      "       [-0.06035285],\n",
      "       [-0.06040118],\n",
      "       [-0.06042723],\n",
      "       [-0.06054851],\n",
      "       [-0.0604121 ],\n",
      "       [-0.06043033],\n",
      "       [-0.06038451],\n",
      "       [-0.06029445],\n",
      "       [-0.06017066],\n",
      "       [-0.06052907],\n",
      "       [-0.06044604],\n",
      "       [-0.06052421],\n",
      "       [-0.06051583],\n",
      "       [-0.06054223],\n",
      "       [-0.06034093],\n",
      "       [-0.06041467],\n",
      "       [-0.06043953],\n",
      "       [-0.06044822],\n",
      "       [-0.06046212],\n",
      "       [-0.06045858],\n",
      "       [-0.06043556],\n",
      "       [-0.06052686],\n",
      "       [-0.06033069],\n",
      "       [-0.06036488],\n",
      "       [-0.0603335 ],\n",
      "       [-0.06050076],\n",
      "       [-0.06052103],\n",
      "       [-0.06030918],\n",
      "       [-0.06041675],\n",
      "       [-0.06049127],\n",
      "       [-0.0602758 ],\n",
      "       [-0.06048155],\n",
      "       [-0.0603692 ],\n",
      "       [-0.06036974],\n",
      "       [-0.06036108],\n",
      "       [-0.06045769],\n",
      "       [-0.06037388],\n",
      "       [-0.06043765],\n",
      "       [-0.06037996],\n",
      "       [-0.06035545],\n",
      "       [-0.06054383],\n",
      "       [-0.06023603],\n",
      "       [-0.06033088],\n",
      "       [-0.06046898],\n",
      "       [-0.06035142],\n",
      "       [-0.06050823],\n",
      "       [-0.06021462],\n",
      "       [-0.060495  ],\n",
      "       [-0.0604341 ],\n",
      "       [-0.06047043],\n",
      "       [-0.06049612],\n",
      "       [-0.06017066],\n",
      "       [-0.06053863],\n",
      "       [-0.06028803],\n",
      "       [-0.06045155],\n",
      "       [-0.06017066],\n",
      "       [-0.06045294],\n",
      "       [-0.0605036 ],\n",
      "       [-0.06048983],\n",
      "       [-0.06017066],\n",
      "       [-0.0604483 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06035789],\n",
      "       [-0.06017066],\n",
      "       [-0.06053617],\n",
      "       [-0.06052171],\n",
      "       [-0.06048783],\n",
      "       [-0.06046701],\n",
      "       [-0.06058169],\n",
      "       [-0.06049139],\n",
      "       [-0.06017066],\n",
      "       [-0.060336  ],\n",
      "       [-0.06044386],\n",
      "       [-0.06048394],\n",
      "       [-0.06044504],\n",
      "       [-0.06020689],\n",
      "       [-0.06052187],\n",
      "       [-0.06035713],\n",
      "       [-0.06043423],\n",
      "       [-0.06055277],\n",
      "       [-0.0604284 ],\n",
      "       [-0.06050619],\n",
      "       [-0.0604798 ],\n",
      "       [-0.06039365],\n",
      "       [-0.06043132],\n",
      "       [-0.06045326],\n",
      "       [-0.06039983],\n",
      "       [-0.06043437],\n",
      "       [-0.06045587],\n",
      "       [-0.06048409],\n",
      "       [-0.06040418],\n",
      "       [-0.06044022],\n",
      "       [-0.06030072],\n",
      "       [-0.06050417],\n",
      "       [-0.0604939 ],\n",
      "       [-0.06046931],\n",
      "       [-0.06055931],\n",
      "       [-0.06048336],\n",
      "       [-0.06053673],\n",
      "       [-0.06052566],\n",
      "       [-0.06017066],\n",
      "       [-0.06042216],\n",
      "       [-0.06050536],\n",
      "       [-0.06024634],\n",
      "       [-0.06035978],\n",
      "       [-0.06056733],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06044592],\n",
      "       [-0.06043097],\n",
      "       [-0.06050034],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06050424],\n",
      "       [-0.06045188],\n",
      "       [-0.06056298],\n",
      "       [-0.06044196],\n",
      "       [-0.0603684 ],\n",
      "       [-0.06037808],\n",
      "       [-0.06032267],\n",
      "       [-0.06053933],\n",
      "       [-0.06039048],\n",
      "       [-0.06036701],\n",
      "       [-0.06058033],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.0603838 ],\n",
      "       [-0.06031663],\n",
      "       [-0.06017066],\n",
      "       [-0.06035596],\n",
      "       [-0.06048923],\n",
      "       [-0.06042092],\n",
      "       [-0.06043775],\n",
      "       [-0.06017066],\n",
      "       [-0.06043556],\n",
      "       [-0.06029997],\n",
      "       [-0.06017066],\n",
      "       [-0.06017066],\n",
      "       [-0.06050399],\n",
      "       [-0.06041165],\n",
      "       [-0.06047653],\n",
      "       [-0.06045548],\n",
      "       [-0.06048929],\n",
      "       [-0.06045383],\n",
      "       [-0.06042233],\n",
      "       [-0.06017066],\n",
      "       [-0.06044013],\n",
      "       [-0.06035059],\n",
      "       [-0.06047758],\n",
      "       [-0.06049841],\n",
      "       [-0.06046926],\n",
      "       [-0.06039456],\n",
      "       [-0.06050487],\n",
      "       [-0.06041669],\n",
      "       [-0.06050026],\n",
      "       [-0.06044948],\n",
      "       [-0.06042137],\n",
      "       [-0.06036801],\n",
      "       [-0.06042147],\n",
      "       [-0.06046008],\n",
      "       [-0.06040192],\n",
      "       [-0.06052019],\n",
      "       [-0.0603139 ],\n",
      "       [-0.06017066],\n",
      "       [-0.06032657],\n",
      "       [-0.06047115],\n",
      "       [-0.06041035],\n",
      "       [-0.06050259],\n",
      "       [-0.06045648],\n",
      "       [-0.06041133],\n",
      "       [-0.06047963],\n",
      "       [-0.06017066],\n",
      "       [-0.06022162],\n",
      "       [-0.0603763 ],\n",
      "       [-0.0603206 ],\n",
      "       [-0.06036957],\n",
      "       [-0.06040055],\n",
      "       [-0.06029294],\n",
      "       [-0.06047309],\n",
      "       [-0.06017066],\n",
      "       [-0.06050154],\n",
      "       [-0.06017066],\n",
      "       [-0.06050896],\n",
      "       [-0.06044341],\n",
      "       [-0.0605429 ],\n",
      "       [-0.06048359],\n",
      "       [-0.06052449]])]\n",
      "Before training- loss value [[693.63164393]] accuracy 0.5\n"
     ]
    }
   ],
   "source": [
    "# Build a model\n",
    "model = mlp(layersizes=[2, 4, 3, 1],\n",
    "    activations=[relu, relu, sigmoid],\n",
    "    derivatives=[drelu, drelu, dsigmoid],\n",
    "    lossderiv=d_cross_entropy)\n",
    "model.initialize()\n",
    "\n",
    "yhat = model.forward(X)\n",
    "loss = cross_entropy(y, yhat)\n",
    "score = accuracy_score(y, (yhat > 0.5))\n",
    "\n",
    "print(f'Before training- loss value {loss} accuracy {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7660de0c-8213-47bd-ad94-2df97c38a839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Loss value: 693.6316 - Accuracy: 0.5000\n",
      "Iteration 1 - Loss value: 693.6228 - Accuracy: 0.5000\n",
      "Iteration 2 - Loss value: 693.6159 - Accuracy: 0.5000\n",
      "Iteration 3 - Loss value: 693.6102 - Accuracy: 0.5000\n",
      "Iteration 4 - Loss value: 693.6052 - Accuracy: 0.5000\n",
      "Iteration 5 - Loss value: 693.6006 - Accuracy: 0.5000\n",
      "Iteration 6 - Loss value: 693.5965 - Accuracy: 0.5000\n",
      "Iteration 7 - Loss value: 693.5927 - Accuracy: 0.5000\n",
      "Iteration 8 - Loss value: 693.5891 - Accuracy: 0.5000\n",
      "Iteration 9 - Loss value: 693.5858 - Accuracy: 0.5000\n",
      "Iteration 10 - Loss value: 693.5827 - Accuracy: 0.5000\n",
      "Iteration 11 - Loss value: 693.5798 - Accuracy: 0.5000\n",
      "Iteration 12 - Loss value: 693.5770 - Accuracy: 0.5000\n",
      "Iteration 13 - Loss value: 693.5743 - Accuracy: 0.5000\n",
      "Iteration 14 - Loss value: 693.5717 - Accuracy: 0.5000\n",
      "Iteration 15 - Loss value: 693.5692 - Accuracy: 0.5000\n",
      "Iteration 16 - Loss value: 693.5668 - Accuracy: 0.5000\n",
      "Iteration 17 - Loss value: 693.5644 - Accuracy: 0.5000\n",
      "Iteration 18 - Loss value: 693.5621 - Accuracy: 0.5000\n",
      "Iteration 19 - Loss value: 693.5598 - Accuracy: 0.5000\n",
      "Iteration 20 - Loss value: 693.5576 - Accuracy: 0.5000\n",
      "Iteration 21 - Loss value: 693.5554 - Accuracy: 0.5000\n",
      "Iteration 22 - Loss value: 693.5532 - Accuracy: 0.5000\n",
      "Iteration 23 - Loss value: 693.5511 - Accuracy: 0.5000\n",
      "Iteration 24 - Loss value: 693.5489 - Accuracy: 0.5000\n",
      "Iteration 25 - Loss value: 693.5469 - Accuracy: 0.5000\n",
      "Iteration 26 - Loss value: 693.5448 - Accuracy: 0.5000\n",
      "Iteration 27 - Loss value: 693.5428 - Accuracy: 0.5000\n",
      "Iteration 28 - Loss value: 693.5407 - Accuracy: 0.5000\n",
      "Iteration 29 - Loss value: 693.5387 - Accuracy: 0.5000\n",
      "Iteration 30 - Loss value: 693.5367 - Accuracy: 0.5000\n",
      "Iteration 31 - Loss value: 693.5347 - Accuracy: 0.5000\n",
      "Iteration 32 - Loss value: 693.5328 - Accuracy: 0.5000\n",
      "Iteration 33 - Loss value: 693.5308 - Accuracy: 0.5000\n",
      "Iteration 34 - Loss value: 693.5289 - Accuracy: 0.5000\n",
      "Iteration 35 - Loss value: 693.5269 - Accuracy: 0.5000\n",
      "Iteration 36 - Loss value: 693.5250 - Accuracy: 0.5000\n",
      "Iteration 37 - Loss value: 693.5231 - Accuracy: 0.5000\n",
      "Iteration 38 - Loss value: 693.5212 - Accuracy: 0.5000\n",
      "Iteration 39 - Loss value: 693.5193 - Accuracy: 0.5000\n",
      "Iteration 40 - Loss value: 693.5174 - Accuracy: 0.5000\n",
      "Iteration 41 - Loss value: 693.5156 - Accuracy: 0.5000\n",
      "Iteration 42 - Loss value: 693.5137 - Accuracy: 0.5000\n",
      "Iteration 43 - Loss value: 693.5119 - Accuracy: 0.5000\n",
      "Iteration 44 - Loss value: 693.5100 - Accuracy: 0.5000\n",
      "Iteration 45 - Loss value: 693.5082 - Accuracy: 0.5000\n",
      "Iteration 46 - Loss value: 693.5063 - Accuracy: 0.5000\n",
      "Iteration 47 - Loss value: 693.5045 - Accuracy: 0.5000\n",
      "Iteration 48 - Loss value: 693.5026 - Accuracy: 0.5000\n",
      "Iteration 49 - Loss value: 693.5008 - Accuracy: 0.5000\n",
      "Iteration 50 - Loss value: 693.4990 - Accuracy: 0.5000\n",
      "Iteration 51 - Loss value: 693.4972 - Accuracy: 0.5000\n",
      "Iteration 52 - Loss value: 693.4953 - Accuracy: 0.5000\n",
      "Iteration 53 - Loss value: 693.4935 - Accuracy: 0.5000\n",
      "Iteration 54 - Loss value: 693.4916 - Accuracy: 0.5000\n",
      "Iteration 55 - Loss value: 693.4898 - Accuracy: 0.5000\n",
      "Iteration 56 - Loss value: 693.4879 - Accuracy: 0.5000\n",
      "Iteration 57 - Loss value: 693.4861 - Accuracy: 0.5000\n",
      "Iteration 58 - Loss value: 693.4842 - Accuracy: 0.5000\n",
      "Iteration 59 - Loss value: 693.4823 - Accuracy: 0.5000\n",
      "Iteration 60 - Loss value: 693.4803 - Accuracy: 0.5000\n",
      "Iteration 61 - Loss value: 693.4783 - Accuracy: 0.5000\n",
      "Iteration 62 - Loss value: 693.4763 - Accuracy: 0.5000\n",
      "Iteration 63 - Loss value: 693.4743 - Accuracy: 0.5000\n",
      "Iteration 64 - Loss value: 693.4721 - Accuracy: 0.5000\n",
      "Iteration 65 - Loss value: 693.4699 - Accuracy: 0.5000\n",
      "Iteration 66 - Loss value: 693.4676 - Accuracy: 0.5000\n",
      "Iteration 67 - Loss value: 693.4652 - Accuracy: 0.5000\n",
      "Iteration 68 - Loss value: 693.4627 - Accuracy: 0.5000\n",
      "Iteration 69 - Loss value: 693.4600 - Accuracy: 0.5000\n",
      "Iteration 70 - Loss value: 693.4572 - Accuracy: 0.5000\n",
      "Iteration 71 - Loss value: 693.4541 - Accuracy: 0.5000\n",
      "Iteration 72 - Loss value: 693.4508 - Accuracy: 0.5000\n",
      "Iteration 73 - Loss value: 693.4471 - Accuracy: 0.5000\n",
      "Iteration 74 - Loss value: 693.4431 - Accuracy: 0.5000\n",
      "Iteration 75 - Loss value: 693.4387 - Accuracy: 0.5000\n",
      "Iteration 76 - Loss value: 693.4337 - Accuracy: 0.5000\n",
      "Iteration 77 - Loss value: 693.4280 - Accuracy: 0.5000\n",
      "Iteration 78 - Loss value: 693.4214 - Accuracy: 0.5000\n",
      "Iteration 79 - Loss value: 693.4132 - Accuracy: 0.5000\n",
      "Iteration 80 - Loss value: 693.4029 - Accuracy: 0.5000\n",
      "Iteration 81 - Loss value: 693.3891 - Accuracy: 0.5000\n",
      "Iteration 82 - Loss value: 693.3722 - Accuracy: 0.5000\n",
      "Iteration 83 - Loss value: 693.3549 - Accuracy: 0.5000\n",
      "Iteration 84 - Loss value: 693.3365 - Accuracy: 0.5000\n",
      "Iteration 85 - Loss value: 693.3167 - Accuracy: 0.5000\n",
      "Iteration 86 - Loss value: 693.2955 - Accuracy: 0.5000\n",
      "Iteration 87 - Loss value: 693.2727 - Accuracy: 0.5000\n",
      "Iteration 88 - Loss value: 693.2474 - Accuracy: 0.5000\n",
      "Iteration 89 - Loss value: 693.2194 - Accuracy: 0.5000\n",
      "Iteration 90 - Loss value: 693.1883 - Accuracy: 0.5000\n",
      "Iteration 91 - Loss value: 693.1544 - Accuracy: 0.5000\n",
      "Iteration 92 - Loss value: 693.1177 - Accuracy: 0.5000\n",
      "Iteration 93 - Loss value: 693.0776 - Accuracy: 0.5000\n",
      "Iteration 94 - Loss value: 693.0347 - Accuracy: 0.5000\n",
      "Iteration 95 - Loss value: 692.9890 - Accuracy: 0.5000\n",
      "Iteration 96 - Loss value: 692.9403 - Accuracy: 0.5000\n",
      "Iteration 97 - Loss value: 692.8885 - Accuracy: 0.5000\n",
      "Iteration 98 - Loss value: 692.8323 - Accuracy: 0.5000\n",
      "Iteration 99 - Loss value: 692.7728 - Accuracy: 0.5000\n",
      "Iteration 100 - Loss value: 692.7086 - Accuracy: 0.5000\n",
      "Iteration 101 - Loss value: 692.6406 - Accuracy: 0.5000\n",
      "Iteration 102 - Loss value: 692.5687 - Accuracy: 0.5000\n",
      "Iteration 103 - Loss value: 692.5036 - Accuracy: 0.5000\n",
      "Iteration 104 - Loss value: 692.4098 - Accuracy: 0.5000\n",
      "Iteration 105 - Loss value: 692.3565 - Accuracy: 0.5000\n",
      "Iteration 106 - Loss value: 692.2340 - Accuracy: 0.5000\n",
      "Iteration 107 - Loss value: 692.1640 - Accuracy: 0.5000\n",
      "Iteration 108 - Loss value: 692.0855 - Accuracy: 0.5000\n",
      "Iteration 109 - Loss value: 692.0905 - Accuracy: 0.5000\n",
      "Iteration 110 - Loss value: 691.8991 - Accuracy: 0.5000\n",
      "Iteration 111 - Loss value: 691.6199 - Accuracy: 0.5000\n",
      "Iteration 112 - Loss value: 691.5204 - Accuracy: 0.5000\n",
      "Iteration 113 - Loss value: 691.6949 - Accuracy: 0.5000\n",
      "Iteration 114 - Loss value: 691.4631 - Accuracy: 0.5000\n",
      "Iteration 115 - Loss value: 690.9786 - Accuracy: 0.5000\n",
      "Iteration 116 - Loss value: 690.9513 - Accuracy: 0.5000\n",
      "Iteration 117 - Loss value: 691.5139 - Accuracy: 0.5000\n",
      "Iteration 118 - Loss value: 691.0848 - Accuracy: 0.5000\n",
      "Iteration 119 - Loss value: 690.1894 - Accuracy: 0.5000\n",
      "Iteration 120 - Loss value: 690.1347 - Accuracy: 0.5000\n",
      "Iteration 121 - Loss value: 691.0210 - Accuracy: 0.5000\n",
      "Iteration 122 - Loss value: 690.6194 - Accuracy: 0.5000\n",
      "Iteration 123 - Loss value: 689.2013 - Accuracy: 0.5000\n",
      "Iteration 124 - Loss value: 689.7570 - Accuracy: 0.5000\n",
      "Iteration 125 - Loss value: 690.8102 - Accuracy: 0.4150\n",
      "Iteration 126 - Loss value: 689.8843 - Accuracy: 0.5000\n",
      "Iteration 127 - Loss value: 688.5221 - Accuracy: 0.5000\n",
      "Iteration 128 - Loss value: 689.9886 - Accuracy: 0.5000\n",
      "Iteration 129 - Loss value: 688.6149 - Accuracy: 0.6260\n",
      "Iteration 130 - Loss value: 689.6017 - Accuracy: 0.5000\n",
      "Iteration 131 - Loss value: 687.2145 - Accuracy: 0.7150\n",
      "Iteration 132 - Loss value: 688.4300 - Accuracy: 0.5100\n",
      "Iteration 133 - Loss value: 687.8981 - Accuracy: 0.8740\n",
      "Iteration 134 - Loss value: 688.3107 - Accuracy: 0.5440\n",
      "Iteration 135 - Loss value: 685.7962 - Accuracy: 0.8780\n",
      "Iteration 136 - Loss value: 687.7249 - Accuracy: 0.5580\n",
      "Iteration 137 - Loss value: 685.7163 - Accuracy: 0.7100\n",
      "Iteration 138 - Loss value: 687.0469 - Accuracy: 0.6260\n",
      "Iteration 139 - Loss value: 681.3303 - Accuracy: 0.8140\n",
      "Iteration 140 - Loss value: 682.2185 - Accuracy: 0.7710\n",
      "Iteration 141 - Loss value: 681.7767 - Accuracy: 0.7120\n",
      "Iteration 142 - Loss value: 691.9933 - Accuracy: 0.5790\n",
      "Iteration 143 - Loss value: 689.9621 - Accuracy: 0.5590\n",
      "Iteration 144 - Loss value: 679.8300 - Accuracy: 0.8080\n",
      "Iteration 145 - Loss value: 681.4812 - Accuracy: 0.5510\n",
      "Iteration 146 - Loss value: 684.7128 - Accuracy: 0.6840\n",
      "Iteration 147 - Loss value: 674.6798 - Accuracy: 0.7410\n",
      "Iteration 148 - Loss value: 698.6391 - Accuracy: 0.5670\n",
      "Iteration 149 - Loss value: 684.2845 - Accuracy: 0.6970\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento por épocas\n",
    "n_epochs = 150\n",
    "learning_rate = 0.005\n",
    "\n",
    "for n in range(n_epochs):\n",
    "    model.forward(X)\n",
    "    yhat = model.a[-1]\n",
    "    model.backward(y, yhat)\n",
    "    model.update(learning_rate)\n",
    "    \n",
    "    loss = cross_entropy(y, yhat)\n",
    "    score = accuracy_score(y, (yhat > 0.5))\n",
    "    \n",
    "    print(f'Iteration {n} - Loss value: {loss.item():.4f} - Accuracy: {score:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
